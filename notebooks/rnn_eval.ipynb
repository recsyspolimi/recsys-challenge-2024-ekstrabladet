{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 09:30:10.718174: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-19 09:30:10.780875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 09:30:11.816364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "import numpy as np\n",
    "import logging\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/ubuntu/experiments/rnn_conv_all_2024-06-17_14-48-01/info.json') as info_file:\n",
    "    info = json.load(info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polimi.utils.tf_models.utils.build_sequences import build_history_seq, build_sequences_seq_iterator, N_CATEGORY, N_SENTIMENT_LABEL, N_SUBCATEGORY, N_TOPICS, N_HOUR_GROUP, N_WEEKDAY\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder\n",
    "import joblib\n",
    "\n",
    "history = pl.read_parquet('/home/ubuntu/dataset/ebnerd_small/validation/history.parquet')\n",
    "articles = pl.read_parquet('/home/ubuntu/dataset/ebnerd_small/articles.parquet')\n",
    "\n",
    "history_seq = build_history_seq(history, articles)\n",
    "\n",
    "behaviors = pl.read_parquet('/home/ubuntu/dset_complete/validation_ds.parquet')\n",
    "\n",
    "categorical_columns = info['categorical_columns']\n",
    "numerical_columns = info['numerical_columns']\n",
    "\n",
    "behaviors_pandas = behaviors.to_pandas()\n",
    "\n",
    "xformer = joblib.load('/home/ubuntu/experiments/rnn_conv_all_2024-06-17_14-48-01/power_transformer.joblib')\n",
    "behaviors_pandas[numerical_columns] = behaviors_pandas[numerical_columns].replace([-np.inf, np.inf], np.nan).fillna(0)\n",
    "behaviors_pandas[numerical_columns] = xformer.transform(behaviors_pandas[numerical_columns]).astype(np.float32)\n",
    "\n",
    "encoder = joblib.load('/home/ubuntu/experiments/rnn_conv_all_2024-06-17_14-48-01/ordinal_encoder.joblib')\n",
    "for i, f in enumerate(categorical_columns):\n",
    "    behaviors_pandas[f] = behaviors_pandas[f].astype(str).fillna('NA')\n",
    "    categories_val = list(behaviors_pandas[f].unique())\n",
    "    unknown_categories = [x for x in categories_val if x not in encoder.categories_[i]]\n",
    "    behaviors_pandas[f] = behaviors_pandas[f].replace(list(unknown_categories), 'NA')\n",
    "behaviors_pandas[categorical_columns] = encoder.transform(behaviors_pandas[categorical_columns]).astype(np.int16)\n",
    "behaviors = behaviors.select(['target', 'user_id', 'impression_id', 'article']).hstack(pl.from_pandas(behaviors_pandas[numerical_columns + categorical_columns]))\n",
    "\n",
    "vocabulary_sizes = {\n",
    "    feature: len(encoder.categories_[i]) for i, feature in enumerate(categorical_columns)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences_cls_iterator_test(history_seq: pl.DataFrame, behaviors: pl.DataFrame, window: int, \n",
    "                                      categorical_columns: list[str], numerical_columns: list[str], record_order: list):\n",
    "    all_features = history_seq.drop('user_id').columns\n",
    "    \n",
    "    multi_one_hot_cols = ['topics', 'subcategory']\n",
    "    categorical_cols = ['category', 'weekday', 'hour_group', 'sentiment_label']\n",
    "    caterical_cols_num_classes = {\n",
    "        'category': N_CATEGORY + 1,#+1 to handle null values\n",
    "        'weekday': N_WEEKDAY,\n",
    "        'hour_group': N_HOUR_GROUP,\n",
    "        'sentiment_label': N_SENTIMENT_LABEL + 1 #+1 to handle null\n",
    "    }\n",
    "    #it can be hardcoded if needed\n",
    "    name_idx_dict = {key: [i for i, col in enumerate(all_features) if col.startswith(key)] for key in multi_one_hot_cols + categorical_cols}\n",
    "    numerical_cols = ['scroll_percentage', 'read_time', 'premium']\n",
    "    name_idx_dict['numerical'] = [i for i, col in enumerate(all_features) if col in numerical_cols]\n",
    "    \n",
    "    mask = 0\n",
    "    history_seq_trucated = history_seq.with_columns(\n",
    "        pl.all().exclude('user_id').list.reverse().list.eval(pl.element().extend_constant(mask, window)).list.reverse().list.tail(window).name.keep()\n",
    "    )\n",
    "    \n",
    "    len_numerical = len(numerical_columns)\n",
    "    \n",
    "    for user_history in history_seq_trucated.to_numpy():\n",
    "        \n",
    "        user_id = user_history[0]\n",
    "        x = np.array([np.array(x_i) for x_i in user_history[1:]])\n",
    "        res_x = {}\n",
    "        for key, idx in name_idx_dict.items():\n",
    "            res_x[f'input_{key}'] = x[idx, :].T\n",
    "         \n",
    "        behaviors_user = behaviors.filter(pl.col('user_id') == user_id)\n",
    "        X = behaviors_user.select(numerical_columns + categorical_columns).to_numpy()\n",
    "        y = behaviors_user.select('target').to_numpy().flatten()\n",
    "        impression_ids = behaviors_user['impression_id'].to_list()\n",
    "        articles = behaviors_user['article'].to_list()\n",
    "        for i in range(behaviors_user.shape[0]):\n",
    "            record_order.append([impression_ids[i], user_id, articles[i], y[i]])\n",
    "            yield {\n",
    "                'numerical_columns': X[i, :len_numerical],\n",
    "                **{c: X[i, j+len_numerical] for j, c in enumerate(categorical_columns)},\n",
    "                **res_x\n",
    "            }, y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polimi.utils.tf_models.utils.build_sequences import build_sequences_cls_iterator\n",
    "\n",
    "record_order = []\n",
    "window = 30\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda : build_sequences_cls_iterator_test(history_seq, behaviors, window=window, numerical_columns=numerical_columns,\n",
    "                                               categorical_columns=categorical_columns, record_order=record_order),\n",
    "    output_signature=(\n",
    "        {\n",
    "            'numerical_columns': tf.TensorSpec(shape=(len(numerical_columns),), dtype=tf.float32), # behaviors numerical columns\n",
    "            **{c: tf.TensorSpec(shape=(), dtype=tf.int16) for c in categorical_columns}, # behaviors categorical columns\n",
    "            'input_topics': tf.TensorSpec(shape=(window,N_TOPICS+1), dtype=tf.int32), # history topics sequence\n",
    "            'input_category': tf.TensorSpec(shape=(window, 1), dtype=tf.int32), # history category sequence\n",
    "            'input_subcategory': tf.TensorSpec(shape=(window, N_SUBCATEGORY+1), dtype=tf.int32), # history subcategory sequence\n",
    "            'input_weekday': tf.TensorSpec(shape=(window, 1), dtype=tf.int32), # history weekday sequence\n",
    "            'input_hour_group': tf.TensorSpec(shape=(window, 1), dtype=tf.int32), # history hour_group sequence\n",
    "            'input_sentiment_label': tf.TensorSpec(shape=(window, 1), dtype=tf.int32), # history sentiment_label sequence\n",
    "            'input_numerical': tf.TensorSpec(shape=(window, 3), dtype=tf.float32), # history (premium, read_time, scroll_percentage) sequence\n",
    "        },\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32), # target\n",
    "    )\n",
    ").batch(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polimi.utils.tf_models import TemporalHistorySequenceModel, TemporalHistoryClassificationModel\n",
    "\n",
    "model = TemporalHistoryClassificationModel(\n",
    "    categorical_features=categorical_columns,\n",
    "    numerical_features=numerical_columns,\n",
    "    vocabulary_sizes=vocabulary_sizes,\n",
    "    seq_embedding_dims={\n",
    "        'input_topics': (N_TOPICS + 1, 20, True),\n",
    "        'input_subcategory': (N_SUBCATEGORY + 1, 20, True),\n",
    "        'input_category': (N_CATEGORY + 1, 20, False),\n",
    "        'input_weekday': (N_WEEKDAY, 3, False),\n",
    "        'input_hour_group': (N_HOUR_GROUP, 3, False),\n",
    "        'input_sentiment_label': (N_SENTIMENT_LABEL + 1, 2, False)\n",
    "    },\n",
    "    seq_numerical_features=['scroll_percentage', 'read_time', 'premium'],\n",
    "    n_recurrent_layers=1,\n",
    "    recurrent_embedding_dim=128,\n",
    "    l1_lambda=1e-4,\n",
    "    l2_lambda=1e-4,\n",
    "    dense_n_layers=2,\n",
    "    dense_start_units=256,\n",
    "    dense_units_decay=2,\n",
    "    dense_activation='swish',\n",
    "    dense_dropout_rate=0.2,\n",
    ")\n",
    "\n",
    "model._build()\n",
    "model.model.load_weights('/home/ubuntu/experiments/rnn_conv_all_2024-06-17_14-48-01/checkpoints/checkpoint.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5721/5721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3506s\u001b[0m 612ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-06-17 16:03:28.559503: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(validation_dataset, batch_size=512).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8030083470322491"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastauc.fastauc.fast_auc import fast_numba_auc\n",
    "\n",
    "evaluation_ds = pl.DataFrame(record_order, schema=['impression_id', 'user_id', 'article', 'target'])\n",
    "evaluation_ds = evaluation_ds.with_columns(pl.Series(predictions).alias('prediction'))\n",
    "evaluation_ds = evaluation_ds.group_by('impression_id').agg(pl.col('target'), pl.col('prediction'))\n",
    "\n",
    "auc = np.mean(\n",
    "    [fast_numba_auc(np.array(y_t).astype(bool), np.array(y_s).astype(np.float32)) \n",
    "        for y_t, y_s in zip(evaluation_ds['target'].to_list(), \n",
    "                            evaluation_ds['prediction'].to_list())]\n",
    ")\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/ubuntu/experiments/rnn_conv_all_2024-06-18_18-44-22/info.json') as info_file:\n",
    "    info = json.load(info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polimi.utils.tf_models.utils.build_sequences import build_history_seq, build_sequences_seq_iterator, N_CATEGORY, N_SENTIMENT_LABEL, N_SUBCATEGORY, N_TOPICS, N_HOUR_GROUP, N_WEEKDAY\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder\n",
    "import joblib\n",
    "\n",
    "history = pl.read_parquet('/home/ubuntu/dataset/ebnerd_small/validation/history.parquet')\n",
    "articles = pl.read_parquet('/home/ubuntu/dataset/ebnerd_small/articles.parquet')\n",
    "\n",
    "history_seq = build_history_seq(history, articles)\n",
    "\n",
    "behaviors = pl.read_parquet('/home/ubuntu/dset_complete/validation_ds.parquet')\n",
    "\n",
    "categorical_columns = info['categorical_columns']\n",
    "numerical_columns = info['numerical_columns']\n",
    "\n",
    "behaviors_pandas = behaviors.to_pandas()\n",
    "\n",
    "xformer = joblib.load('/home/ubuntu/experiments/rnn_conv_all_2024-06-18_18-44-22/power_transformer.joblib')\n",
    "behaviors_pandas[numerical_columns] = behaviors_pandas[numerical_columns].replace([-np.inf, np.inf], np.nan).fillna(0)\n",
    "behaviors_pandas[numerical_columns] = xformer.transform(behaviors_pandas[numerical_columns]).astype(np.float32)\n",
    "\n",
    "encoder = joblib.load('/home/ubuntu/experiments/rnn_conv_all_2024-06-18_18-44-22/ordinal_encoder.joblib')\n",
    "for i, f in enumerate(categorical_columns):\n",
    "    behaviors_pandas[f] = behaviors_pandas[f].astype(str).fillna('NA')\n",
    "    categories_val = list(behaviors_pandas[f].unique())\n",
    "    unknown_categories = [x for x in categories_val if x not in encoder.categories_[i]]\n",
    "    behaviors_pandas[f] = behaviors_pandas[f].replace(list(unknown_categories), 'NA')\n",
    "behaviors_pandas[categorical_columns] = encoder.transform(behaviors_pandas[categorical_columns]).astype(np.int16)\n",
    "behaviors = behaviors.select(['target', 'user_id', 'impression_id', 'article']).hstack(pl.from_pandas(behaviors_pandas[numerical_columns + categorical_columns]))\n",
    "\n",
    "vocabulary_sizes = {\n",
    "    feature: len(encoder.categories_[i]) for i, feature in enumerate(categorical_columns)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences_cls_iterator_test(history_seq: pl.DataFrame, behaviors: pl.DataFrame, window: int, \n",
    "                                      categorical_columns: list[str], numerical_columns: list[str], record_order: list):\n",
    "    all_features = history_seq.drop('user_id').columns\n",
    "    \n",
    "    multi_one_hot_cols = ['topics', 'subcategory']\n",
    "    categorical_cols = ['category', 'weekday', 'hour_group', 'sentiment_label']\n",
    "    caterical_cols_num_classes = {\n",
    "        'category': N_CATEGORY + 1,#+1 to handle null values\n",
    "        'weekday': N_WEEKDAY,\n",
    "        'hour_group': N_HOUR_GROUP,\n",
    "        'sentiment_label': N_SENTIMENT_LABEL + 1 #+1 to handle null\n",
    "    }\n",
    "    #it can be hardcoded if needed\n",
    "    name_idx_dict = {key: [i for i, col in enumerate(all_features) if col.startswith(key)] for key in multi_one_hot_cols + categorical_cols}\n",
    "    numerical_cols = ['scroll_percentage', 'read_time', 'premium']\n",
    "    name_idx_dict['numerical'] = [i for i, col in enumerate(all_features) if col in numerical_cols]\n",
    "    \n",
    "    mask = 0\n",
    "    history_seq_trucated = history_seq.with_columns(\n",
    "        pl.all().exclude('user_id').list.reverse().list.eval(pl.element().extend_constant(mask, window)).list.reverse().list.tail(window).name.keep()\n",
    "    )\n",
    "    \n",
    "    len_numerical = len(numerical_columns)\n",
    "    \n",
    "    for user_history in history_seq_trucated.to_numpy():\n",
    "        \n",
    "        user_id = user_history[0]\n",
    "        x = np.array([np.array(x_i) for x_i in user_history[1:]])\n",
    "        res_x = {}\n",
    "        for key, idx in name_idx_dict.items():\n",
    "            res_x[f'input_{key}'] = x[idx, :].T\n",
    "         \n",
    "        behaviors_user = behaviors.filter(pl.col('user_id') == user_id)\n",
    "        X = behaviors_user.select(numerical_columns + categorical_columns).to_numpy()\n",
    "        y = behaviors_user.select('target').to_numpy().flatten()\n",
    "        impression_ids = behaviors_user['impression_id'].to_list()\n",
    "        articles = behaviors_user['article'].to_list()\n",
    "        for i in range(behaviors_user.shape[0]):\n",
    "            record_order.append([impression_ids[i], user_id, articles[i], y[i]])\n",
    "            yield {\n",
    "                'numerical_columns': X[i, :len_numerical],\n",
    "                **{c: X[i, j+len_numerical] for j, c in enumerate(categorical_columns)},\n",
    "                **res_x\n",
    "            }, y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polimi.utils.tf_models.utils.build_sequences import build_sequences_cls_iterator\n",
    "\n",
    "record_order = []\n",
    "window = 30\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda : build_sequences_cls_iterator_test(history_seq, behaviors, window=window, numerical_columns=numerical_columns,\n",
    "                                               categorical_columns=categorical_columns, record_order=record_order),\n",
    "    output_signature=(\n",
    "        {\n",
    "            'numerical_columns': tf.TensorSpec(shape=(len(numerical_columns),), dtype=tf.float32), # behaviors numerical columns\n",
    "            **{c: tf.TensorSpec(shape=(), dtype=tf.int16) for c in categorical_columns}, # behaviors categorical columns\n",
    "            'input_topics': tf.TensorSpec(shape=(window,N_TOPICS+1), dtype=tf.int32), # history topics sequence\n",
    "            'input_category': tf.TensorSpec(shape=(window, 1), dtype=tf.int32), # history category sequence\n",
    "            'input_subcategory': tf.TensorSpec(shape=(window, N_SUBCATEGORY+1), dtype=tf.int32), # history subcategory sequence\n",
    "            'input_weekday': tf.TensorSpec(shape=(window, 1), dtype=tf.int32), # history weekday sequence\n",
    "            'input_hour_group': tf.TensorSpec(shape=(window, 1), dtype=tf.int32), # history hour_group sequence\n",
    "            'input_sentiment_label': tf.TensorSpec(shape=(window, 1), dtype=tf.int32), # history sentiment_label sequence\n",
    "            'input_numerical': tf.TensorSpec(shape=(window, 3), dtype=tf.float32), # history (premium, read_time, scroll_percentage) sequence\n",
    "        },\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32), # target\n",
    "    )\n",
    ").batch(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polimi.utils.tf_models import TemporalConvolutionalHistoryClassificationModel\n",
    "\n",
    "model = TemporalConvolutionalHistoryClassificationModel(\n",
    "    categorical_features=categorical_columns,\n",
    "    numerical_features=numerical_columns,\n",
    "    vocabulary_sizes=vocabulary_sizes,\n",
    "    seq_embedding_dims={\n",
    "        'input_topics': (N_TOPICS + 1, 20, True),\n",
    "        'input_subcategory': (N_SUBCATEGORY + 1, 20, True),\n",
    "        'input_category': (N_CATEGORY + 1, 20, False),\n",
    "        'input_weekday': (N_WEEKDAY, 3, False),\n",
    "        'input_hour_group': (N_HOUR_GROUP, 3, False),\n",
    "        'input_sentiment_label': (N_SENTIMENT_LABEL + 1, 2, False)\n",
    "    },\n",
    "    seq_numerical_features=['scroll_percentage', 'read_time', 'premium'],\n",
    "    window_size=window,\n",
    "    n_conv_layers=5,\n",
    "    conv_filters=128,\n",
    "    kernel_size=2,\n",
    "    conv_activation='swish',\n",
    "    l1_lambda=1e-4,\n",
    "    l2_lambda=1e-4,\n",
    "    dropout_rate=0.2,\n",
    "    dense_n_layers=4,\n",
    "    dense_start_units=384,\n",
    "    dense_units_decay=2,\n",
    "    dense_activation='swish',\n",
    ")\n",
    "\n",
    "model._build()\n",
    "model.model.load_weights('/home/ubuntu/experiments/rnn_conv_all_2024-06-18_18-44-22/checkpoints/checkpoint.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5721/5721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3873s\u001b[0m 676ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-06-19 10:37:37.538809: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(validation_dataset, batch_size=512).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8128420126354572"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastauc.fastauc.fast_auc import fast_numba_auc\n",
    "\n",
    "evaluation_ds = pl.DataFrame(record_order, schema=['impression_id', 'user_id', 'article', 'target'])\n",
    "evaluation_ds = evaluation_ds.with_columns(pl.Series(predictions).alias('prediction'))\n",
    "evaluation_ds = evaluation_ds.group_by('impression_id').agg(pl.col('target'), pl.col('prediction'))\n",
    "\n",
    "auc = np.mean(\n",
    "    [fast_numba_auc(np.array(y_t).astype(bool), np.array(y_s).astype(np.float32)) \n",
    "        for y_t, y_s in zip(evaluation_ds['target'].to_list(), \n",
    "                            evaluation_ds['prediction'].to_list())]\n",
    ")\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame(record_order, schema=['impression_id', 'user_id', 'article', 'target']).with_columns(pl.Series(predictions).alias('prediction')).write_parquet('/home/ubuntu/experiments/rnn_conv_all_2024-06-18_18-44-22/predictions.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
